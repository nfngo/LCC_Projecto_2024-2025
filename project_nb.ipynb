{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/logo_UM.png\" alt=\"logo Universidade do Minho\" style=\"display: block; margin: auto; width: 25%\" />\n",
    "<p style=\"text-align: center; font-size: 12px\">Licenciatura em Ciências da Computação</p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "### <center>Unidade Curricular Projeto\n",
    "\n",
    "<br/>\n",
    "\n",
    "# <center>**Armazenamento de imagens e bases de dados vetoriais**\n",
    "####   <center>Maio 2025</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "**a53971** &ensp; Nuno Filipe Norberto Gonçalves de Oliveira\n",
    "\n",
    "**a83783** &ensp; João Pinheiro Vitor\n",
    "\n",
    "**a81820** &ensp; David João Oliveira Gonçalves\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No âmbito da UC Projeto, foi-nos sugerida a criação de um sistema que recolha e armazene ficheiros de imagem num sistema de bases de dados vetorial, para posterior identificação de padrões de imagens.\n",
    "\n",
    "No sentido de ir ao encontro do sugerido, decidimos criar um sistema de reconhecimento facial.\n",
    "\n",
    "Este sistema permite ao utilizador fornecer uma imagem como *input*. O programa irá então identificar as faces presentes na imagem (caso existam) e como resultado apresenta as imagens, presentes na nossa base de dados, com as faces mais semelhantes ao *input*.\n",
    "\n",
    "Ao longo deste documento iremos detalhar quais as tecnologias utilizadas, as etapas do desenvolvimento deste sistema e particularidades da sua implementação.\n",
    "\n",
    "Por fim, faremos uma pequena demonstração do uso da aplicação.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &ensp; Contextualização e motivação\n",
    "\n",
    "O reconhecimento facial é um campo de investigação e desenvolvimento activo há várias décadas, com inúmeras aplicações já implementadas no nosso dia-a-dia. Desde o desbloqueio de smartphones até aos sistemas de vigilância, a tecnologia tem demonstrado o seu potencial e importância crescente.\n",
    "\n",
    "No entanto, existem vários desafios, sejam eles na identificação de características das faces ou no armazenamento das imagens numa base de dados. Lidar com variações na aparência facial devido a diferentes condições de iluminação, ângulos de visão, expressões faciais e oclusões (como óculos ou máscaras) continua a ser um problema complexo. As dificuldades que as bases de dados tradicionais têm no armazenamento de imagens é também um entrave: as imagens podem ser ficheiros grandes, e armazenar um grande volume delas diretamente na base de dados pode levar a um crescimento exponencial do tamanho da mesma, o que pode levar a um baixo desempenho aplicacional.\n",
    "\n",
    "É neste contexto que a utilização de bases de dados vetoriais surge como uma alternativa promissora. Ao representar imagens como vetores num espaço de alta dimensão, onde a distância entre vetores reflete a similaridade entre as imagens, torna-se possível realizar pesquisas mais eficientes e encontrar correspondências mesmo quando as imagens apresentam variações significativas.\n",
    "\n",
    "Este projeto vai de encontro às tendências atuais na área de reconhecimento de padrões e análise de imagem, explorando a cooperação entre técnicas de visão computacional e sistemas de armazenamento. \n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Tecnologias e bibiliotecas utilizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &ensp; Base de dados\n",
    "\n",
    "A seleção de uma base de dados vetorial que se adequasse às necessidades do nosso sistema foi uma etapa relevante no desenvolvimento, dada a sua importância para o armazenamento, indexação e pesquisa dos *embeddings* gerados.\n",
    "<br>Após uma análise comparativa de três soluções – Pinecone, Qdrant e KDB.AI – a nossa escolha recaiu sobre [Qdrant](https://qdrant.tech/) como a ferramenta mais adequada para as necessidades específicas deste projeto.\n",
    "\n",
    "Esta decisão foi fundamentada com base nos seguintes critérios:\n",
    "- Qualidade da documentação\n",
    "\n",
    "- Facilidade de configuração\n",
    "\n",
    "- Possibilidade de uso localmente\n",
    "\n",
    "- Persistência de dados (local)\n",
    "\n",
    "Podemos ver na tabela seguinte os resultados da nossa avaliação:\n",
    "\n",
    "\n",
    "| Critério | Pinecone | Qdrant | KBD.AI |\n",
    "| --- | --- | ---| --- |\n",
    "| **Qualidade da documentação** | Extensa, com exemplos, mas, por vezes, algo confusa | Extensa, com exemplos e bem estruturada | Algo confusa e com menos exemplos que Pinecone e Qdrant |\n",
    "| **Facilidade de configuração** | Simples | Simples | Requer a instalação de mais bibliotecas |\n",
    "| **Uso localmente** | Disponível mas limitado | Disponível | Disponível |\n",
    "| **Persistência de dados (local)** | Não | Sim | Sim |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &ensp; Linguagens e bibliotecas\n",
    "\n",
    "Este projeto foi desenvolvido em Python (versão 3.11).\n",
    "\n",
    "Entre as bibliotecas utilizadas, destacamos as seguintes:\n",
    "- [Transformers](https://huggingface.co/docs/transformers/index): biblioteca de NPL's pré-treinadas, visão computacional, áudio e modelos multimodais para inferência e treino.\n",
    "\n",
    "- [MTCNN (Multi-Task Cascaded Convolutional Neural Network)](https://mtcnn.readthedocs.io/en/latest/): biblioteca robusta para detecção facial, desenvolvida para detetar faces e os seus pontos de referência numa imagem, recorrendo a três CNNs. \n",
    "\n",
    "- [Pillow](https://pillow.readthedocs.io/en/stable/): biblioteca para processamento de imagens.\n",
    "\n",
    "- [Qdrant-client](https://python-client.qdrant.tech/): biblioteca que permite utilizar a API de Qdrant.\n",
    "\n",
    "- [Torch](https://pytorch.org/): biblioteca de *Machine Learning*\n",
    "\n",
    "\n",
    "\n",
    "Foram também utilizadas outras bibliotecas para manipulação de dados numéricos e vetores.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &ensp; Modelos *Vision Transformer*\n",
    "\n",
    "A fase de transformação de imagens em *embeddings* é um passo fundamental para o bom desempenho do nosso sistema de reconhecimento facial, uma vez que a qualidade destas representações vetoriais impacta diretamente a capacidade do sistema em identificar padrões e realizar comparações eficazes. Dada a vasta gama de modelos de *Machine Learning* capazes de executar esta tarefa, a seleção de modelos adequados exigiu a definição de alguns critérios, que nos permitissem escolher aqueles que melhor se ajustassem aos objetivos do projeto.\n",
    "\n",
    "Os critérios de selecção definidos foram os seguintes: \n",
    "- **tarefa para o qual o modelo foi treinado**: focamo-nos em modelos que foram especificamente treinados para a tarefa de *Image Feature Extraction*. Esta especialização garante que o modelo aprendeu a extrair características visuais relevantes das imagens, essenciais para a posterior comparação de similaridade facial.\n",
    "\n",
    "- **tamanho e diversidade do dataset no qual foi treinado**: a capacidade de um modelo reconhecer e categorizar diferentes imagens depende da quantidade e diversidade dos dados nos quais foi treinado. [$^*$](https://developers.google.com/machine-learning/intro-to-ml/supervised)\n",
    "\n",
    "- **resolução das imagens do dataset**: a resolução das imagens que compõem o nosso conjunto de dados situa-se predominantemente entre os 1024x768 e os 2048x1536 pixels. Tendo estes valores em consideração, a nossa abordagem consistiu em extrapolar um tamanho razoável para as faces presentes nas imagens, de maneira que houvesse informação suficiente para capturar os detalhes faciais de cada pessoa, e procurar modelos que tivessem sido treinados com resoluções de imagem que se aproximassem dessa escala facial estimada.\n",
    "\n",
    "- **dimensão do *embedding* gerado**: a dimensão do vetor gerado deve ter em conta um equilíbrio entre a riqueza da informação codificada e a eficiência computacional. Um *embedding* de alta dimensão contém mais informação e detalhes acerca de uma imagem, mas também exige mais recursos de armazenamento e processamento.\n",
    "\n",
    "Com base nestes critérios, realizamos uma pequena análise das diversas opções disponíveis na plataforma [HuggingFace](https://huggingface.co/models).\n",
    "\n",
    "A nossa escolha recaiu sobre os modelos *google/vit-base-patch16-224-in21k* e *facebook/dino-vits16* pelos seguintes motivos:\n",
    "\n",
    "- os dois modelos foram treinados para a tarefa pretendida\n",
    "\n",
    "- ambos geram *embeddings* com a mesma dimensão - 768\n",
    "\n",
    "- foram treinados com imagens de resolução 224x224 pixels, resolução que nos pareceu adequada para o sistema a implementar \n",
    "\n",
    "- o modelo da google foi treinado recorrendo a um dataset com 14 milhões de imagens e 21843 classes/categorias de objetos\n",
    "\n",
    "- o modelo do facebook foi treinado recorrendo a um dataset com aproximadamente 1,3 milhões de imagens e 1000 classes/categorias de objetos\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &ensp; Configuração Inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para a execução bem-sucedida deste notebook, devemos certeficar-nos que os seguintes pré-requisitos são atendidos:\n",
    "\n",
    "- **Disponibilidade das imagens**: as imagens a serem armazenadas na base de dados devem estar localizadas na diretoria **assets**.\n",
    "\n",
    "- **Instalação de Docker**: a plataforma **[Docker](https://docs.docker.com/get-started/get-docker/)** deve estar devidamente instalada\n",
    "\n",
    "- Todos os ***packages*** necessários para a execução do código devem ser previamente instalados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Opcional\n",
    "# Criar ambiente virtual recorrendo ao conda/miniconda\n",
    "conda create -n my_env python=3.11 pip\n",
    "conda activate my_env\n",
    "\n",
    "# Ou recorrendo a virtualenv\n",
    "python=3.11 -m venv venv\n",
    "source venv/bin/activate\n",
    "\n",
    "# Instalar packages\n",
    "pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# Transferência de outros módulos necessários\n",
    "wget https://raw.githubusercontent.com/nfngo/LCC_Projecto_2024-2025/refs/heads/main/qdrant/utils.py\n",
    "wget https://raw.githubusercontent.com/nfngo/LCC_Projecto_2024-2025/refs/heads/main/models/models.py\n",
    "\n",
    "# Transferência da imagem docker de Qdrant\n",
    "docker pull qdrant/qdrant\n",
    "\n",
    "# Inicialização de container de Qdrant\n",
    "!docker run -p 6333:6333 \\\n",
    "            -v $(pwd)/qdrant_storage:/qdrant/storage \\\n",
    "            qdrant/qdrant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Após garantirmos que temos todos os recursos instalados e a funcionar corretamente, importamos as bibliotecas necessárias e estabelecemos uma ligação com a base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import utils as qd\n",
    "import models as mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local dev\n",
    "client = QdrantClient(host=\"localhost\", port=6333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida, criamos uma **colecção**.\n",
    "\n",
    "Uma colecção é um conjunto de pontos na qual podemos efetuar pesquisas. Estes pontos são constituídos por um vetor e um *payload*, que não é mais que uma *feature* de Qdrant que permite associar informação adicional aos vetores. Os vetores de cada ponto que pertença à mesma colecção devem ter a mesma dimensão e são comparados recorrendo a uma métrica de similaridade. \n",
    "\n",
    "Neste projeto, tendo em conta os modelos escolhidos para gerar *embeddings* (representações vetoriais das imagens), cada vetor será composto por 768 valores do tipo *float*.\n",
    "\n",
    "Como métrica de similaridade, optamos pela similaridade por cosseno.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "similarity(A,B) = cos(\\theta) = \\frac{A \\cdot B}{\\|A\\|\\|B\\|}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Esta métrica permite calcular o cosseno do ângulo entre dois vetores, possibilitando saber de forma eficaz se os dois apontam na mesma direcção. Como resultado, obtém-se um valor no intervalo [-1, 1]. Valores próximos de 1 significam que os vetores têm direcções semelhantes e valores próximos de -1 indicam-nos que os vetores apontam em direcções opostas.\n",
    "\n",
    "No contexto de *embeddings* de imagem que representam faces, a direcção representa as propriedades da face. Idealmente, duas imagens da face da mesma pessoa, se considerarmos diferentes condições de luz ou poses diferentes, devem apontar na mesma direcção. \n",
    "\n",
    "É importante, portanto, que seja possível \"descartar\" certas propriedades das imagens, tais como a luminosidade ou o contraste, que influenciam a magnitude dos vetores.\n",
    "<br>Uma vez que no cáculo da similaridade por cosseno se faz uma normalização dos vetores pela sua magnitude, esta métrica torna-se ideal para o pretendido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"image_collection\"\n",
    "\n",
    "# facebook/dino-vitb16 size: 768\n",
    "# Vit Base Patch16 224 In21k size: 768\n",
    "qd.create_collection(client, collection, 768, models.Distance.COSINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &ensp; Processamento de Imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda antes de passarmos à conversão das imagens para *embeddings*, é necessário armazenar informações de cada uma das imagens que pretendemos analisar, que devem estar guardadas na pasta \"assets\".\n",
    "\n",
    "Como estrutura para armazenar esta informação, escolhemos uma lista de dicionários, que nos permite guardar, para cada imagem, os seguintes dados: nome da pessoa presente na imagem, nome do ficheiro e objeto de imagem.\n",
    "\n",
    "De forma a agilizar o processo, o nome de cada ficheiro contém o nome da pessoa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "data = [img_1, img_2, img_3...]\n",
    "\n",
    "img_1 = {\n",
    "        \"name\": ...,\n",
    "        \"file_name\": ...,\n",
    "        \"img_obj\": ...\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "extensions = (\".jpg\", \".jpeg\", \".webp\")\n",
    "\n",
    "def get_images_data(path):\n",
    "    data = []\n",
    "    for file in os.listdir(f\"{path}\"):\n",
    "        if file.endswith(extensions):\n",
    "            file_name = file.split(\".\")[0].lower().replace(\"_\",\" \")\n",
    "            name = \"\".join(c for c in file_name if c.isalpha() or c == ' ')[:-1] \n",
    "            img_data = {\n",
    "                \"name\": name,\n",
    "                \"file_name\": file,\n",
    "                \"img_obj\": Image.open(os.path.join(f\"{path}\",file))\n",
    "            }\n",
    "            data.append(img_data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = get_images_data(f\"{current_directory}/assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As etapas seguintes consistem em recortar a cara da pessoa presente em cada imagem e gerar o respetivo *embedding*.\n",
    "\n",
    "Portanto, é necessário inicializar o modelo ViT que vamos utilizar e o modelo de detecção facial *MTCNN*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "# default model: vit-base-patch16-224-in21k\n",
    "# dino = \"dino-vitb16\"\n",
    "device, processor, model, detector = mdl.init_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, passamos à fase de recorte da face.\n",
    "\n",
    "Nesta etapa, utilizamos o modelo *MTCNN* para obter a posição da face na imagem, através do método *detect_faces*. Este método devolve uma lista de dicionários, cada um deles contendo a posição da caixa delimitadora, pontos de referência faciais e pontuação de confiança de cada face detectada.\n",
    "\n",
    "```python\n",
    "# Exemplo\n",
    "[\n",
    "    {\n",
    "        \"box\": [277, 90, 48, 63],\n",
    "        \"keypoints\": {\n",
    "            \"nose\": [303, 131],\n",
    "            \"mouth_right\": [313, 141],\n",
    "            \"right_eye\": [314, 114],\n",
    "            \"left_eye\": [291, 117],\n",
    "            \"mouth_left\": [296, 143]\n",
    "        },\n",
    "        \"confidence\": 0.9985\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "Após a detecção facial, utilizamos a biblioteca Pillow para isolar e recortar as faces identificadas nas imagens.\n",
    "\n",
    "Com o objetivo de otimizar a fase de *image embedding*, avaliamos as dimensões de cada recorte facial. Definimos uma dimensão alvo de 224x224 pixels. Caso a dimensão do recorte seja significativamente inferior ao pretendido (menos de um terço do tamanho alvo), descartamos o resultado por considerarmos que a informação pode ser insuficiente. Se a dimensão do recorte for superior ao tamanho alvo, realizamos um redimensionamento para dimensões próximas de 224x224 pixels, de forma a uniformizar os dados que vamos fornecer ao modelo ViT. \n",
    "\n",
    "Finalmente, cada recorte facial redimensionado (ou mantido, se as dimensões forem adequadas) é guardado como um ficheiro na pasta \"faces\", mantendo o mesmo nome do ficheiro original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if needed\n",
    "faces_path = \"faces\"\n",
    "if not os.path.exists(faces_path):\n",
    "    os.makedirs(faces_path)\n",
    "\n",
    "\n",
    "destiny_path = f\"{current_directory}/{faces_path}/\"\n",
    "for i, image_data in enumerate(data):\n",
    "    mdl.process_image(image_data['img_obj'], \n",
    "                      detector,\n",
    "                      destiny_path,\n",
    "                      image_data['file_name'])\n",
    "\n",
    "faces_data = get_images_data(f\"{current_directory}/{faces_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em baixo, podemos ver os dados atualizados após o processamento das imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(faces_data), faces_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &ensp; Processo de *Image Embedding*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um dos nossos principais objetivos é extrair características das imagens (faces) e armazená-las na base de dados. Essas características não são mais que representações vetoriais das imagens (***embeddings***) que retêm o conteúdo visual das mesmas e são utilizadas para aumentar a performance de tarefas de computação visual, tais como detecção de objetos e classificação de imagens. \n",
    "\n",
    "<br/>\n",
    "<img src=\"images/embedding.jpg\" alt=\"image to vector embedding\" style=\"display: block; margin: auto; width: 50%;\" />\n",
    "<br/>\n",
    "\n",
    "Para extrair essa representação das características das imagens, iremos utilizar técnicas de ***Image Embedding***. Concretamente, faremos essa conversão recorrendo a modelos ***Vision Transformer (ViT)***.\n",
    "\n",
    "De uma forma resumida, ViTs são modelos de *Machine Learning* avançados que permitem aos computadores \"ver\" e entender informação visual de uma maneira semelhante aos humanos. Baseiam-se numa arquitetura ***Transformer*** para processar imagens e extrair características significativas delas.\n",
    "\n",
    "Para entender como funcionam os ViTs, vamos fazer uma analogia. Imaginemos que temos um puzzle com muitas peças diferentes. Para montar o puzzle, tipicamente olhamos para as peças individuais, para as suas formas e como se encaixam para formar uma imagem completa. \n",
    "\n",
    "ViTs funcionam da mesma maneira. Em vez de analizarem uma imagem completa, os *Vision Transformers* dividem a imagem em pedaços mais pequenos, chamados ***patches***. Cada um desses *patches* é como uma peça do puzzle. Após feita essa divisão, esses pedaços são analizados e processados pelos ViTs.\n",
    "\n",
    "Ao ser feita essa análise, os ViTs identificam padrões importantes como, por exemplo, limites da imagem (*edges*), cores e texturas, e combina-os de maneira a entender a respetiva imagem de forma lógica, nítida e consistente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, para cada imagem guardada na diretoria \"faces\", geramos o *embedding* correspondente, através da função *gen_embeddings*, e acrescentamos esta informação à nossa estrutura de dados. Para o efeito, acrescentamos o campo \"img_embedding\".\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"name\": ...,\n",
    "    \"file_name\": ...,\n",
    "    \"img_obj\": ...,\n",
    "    \"img_embedding\": ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "embeddings = mdl.gen_embeddings(faces_data, processor, device, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &ensp; Armazenamento na base de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta etapa do nosso processo consiste no armazenamento estruturado da informação extraída de cada imagem na base de dados.\n",
    "\n",
    "Conforme já tínhamos mencionado, em Qdrant guardamos informação sob a forma de \"pontos\".\n",
    "\n",
    "```python\n",
    "# Exemplo de um ponto\n",
    "{\n",
    "    \"id\": 129,\n",
    "    \"vector\": [0.1, 0.2, 0.3, 0.4],\n",
    "    \"payload\": {\"color\": \"red\"},\n",
    "}\n",
    "```\n",
    "\n",
    "No contexto do nosso sistema de reconhecimento facial, cada imagem processada será representada como um ponto. O *embedding* gerado para a face detectada na imagem (presente no campo 'img_embedding' da nossa estrutura de dados) será armazenado no campo 'vector' do ponto.\n",
    "\n",
    "Adicionalmente, para preservar informações relevantes sobre cada imagem, utilizaremos o campo 'payload' do ponto. Neste campo, armazenaremos um dicionário contendo as restantes informações associadas à imagem, nomeadamente o nome da pessoa identificada ('name') e o nome do ficheiro da imagem original ('file_name')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to DB\n",
    "for img_data in embeddings:\n",
    "    qd.insert_image_embedding(client, collection, img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma possível representação 2D da nossa base de dados é a seguinte:\n",
    "\n",
    "<br/>\n",
    "<img src=\"images/print_BD.jpg\" alt=\"DB 2D representation\" style=\"display: block; margin: auto; width: 90%;\" />\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &ensp; Procura por similaridade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, chegamos ao ponto central da nossa aplicação: a busca por similaridade facial.\n",
    "\n",
    "Dado uma imagem como *input*, o objetivo é identificar as faces presentes, gerar os seus respetivos *embeddings* e, subsequentemente, procurar da nossa base de dados as imagens de faces mais semelhantes às encontradas na imagem original.\n",
    "\n",
    "O processo inicia-se, portanto, com a detecção de faces na imagem fornecida e a geração dos *embeddings* correspondentes.\n",
    "\n",
    "Esta etapa crucial é implementada na função *gen_embedding_img_to_search*, que tem como resultado duas listas: uma contendo os *embeddings* das faces detectadas e outra com os objetos de imagem dessas mesmas faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_search, searched_faces = mdl.gen_embedding_img_to_search(f\"{current_directory}/img_to_search/taskmaster.jpg\", processor, device, model, detector)\n",
    "img_to_search, searched_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_to_search), len(img_to_search[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, para cada *embedding* das faces detectadas na imagem fornecida, realizamos uma busca pelas X faces mais semelhantes na nossa base de dados. Os resultados dessas buscas são então armazenados na variável *nearest_results*.\n",
    "\n",
    "Como veremos, esta variável contém a resposta a cada consulta efetuada à base de dados, apresentando para cada *embedding*, uma lista de pontos correspondentes aos resultados mais próximos.\n",
    "\n",
    "São de particular interesse para nós os campos 'score' e 'payload', que nos permitem saber a pontuação de similaridade, o nome da pessoa a quem a face pertence e o nome do ficheiro de imagem que lhe é associado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search top X similar results\n",
    "top = 3\n",
    "nearest_results = []\n",
    "for img in img_to_search:\n",
    "    nearest = qd.get_top_x_similar_images(client, collection, top, img)\n",
    "    nearest_results.append(nearest)\n",
    "\n",
    "nearest_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, implementamos a função *print_results* que nos permite consultar os resultados obtidos de uma forma mais legível e gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    msg = f\"{len(results)} faces were found in the provided image\" if len(results) > 1 else f\"{len(results)} face was found in the provided image\"\n",
    "    print(msg)\n",
    "    for j, result in enumerate(results):\n",
    "        # print(f\"{j}: {result.points}\")\n",
    "        points = result.points\n",
    "        print(f\"Face {j+1}:\")\n",
    "        searched_faces[j].show()\n",
    "        for i in range(len(points)):\n",
    "            name = points[i].payload['name']\n",
    "            score = points[i].score\n",
    "            print(f\"Result #{i+1}: {name} was diagnosed with {score * 100} confidence\")\n",
    "            print(f\"This image score was {score}\")\n",
    "            Image.open(f\"faces/{points[i].payload['file_name']}\").show()\n",
    "            print(\"-\" * 50)\n",
    "            print()\n",
    "\n",
    "print_results(nearest_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o objetivo de, no futuro, aumentar as funcionalidades da nossa aplicação, deixamos também disponíveis funções que nos permitem eliminar e consultar pontos da base de dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = qd.get_points_from_collection(client, collection, 5)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = res[0][0].id\n",
    "qd.delete_points_from_collection(client, collection, [id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &ensp; Aplicação *Facial Recognition System*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A última etapa do nosso projeto consistiu em implementar uma aplicação que permitisse, de uma forma simples e intuitiva, fazer uma procura por similaridade entre uma imagem de uma pessoa fornecida pelo utilizador e os dados armazenados na nossa base de dados.\n",
    "\n",
    "Para o efeito, utilizamos a framework de desenvolvimento aplicacional [Streamlit](https://streamlit.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile search_image.py\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "import os\n",
    "import streamlit as st\n",
    "import torch\n",
    "\n",
    "import utils as qd\n",
    "import models as mdl\n",
    "\n",
    "torch.classes.__path__ = []  # Neutralizes the path inspection\n",
    "\n",
    "# Local dev\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "collection = \"image_collection\"\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Initialize models\n",
    "device, processor, model, detector = mdl.init_models()\n",
    "\n",
    "st.title(\"Facial Recognition System\")\n",
    "st.markdown(\"Upload images with different faces and you'll get the most similar ones from our database.\")\n",
    "\n",
    "uploaded_file = st.file_uploader(label = \"Upload some image\",\n",
    "                                 type=[\"jpg\", \"jpeg\", \"webp\"])\n",
    "\n",
    "# Search top X similar results\n",
    "top = st.radio(\n",
    "    \"How many search results do you want?\",\n",
    "    [1, 3, 5],\n",
    ")\n",
    "\n",
    "with st.spinner(\"Waiting for results...\"):\n",
    "    if uploaded_file:\n",
    "        img_to_search, searched_faces = mdl.gen_embedding_img_to_search(uploaded_file, processor, device, model, detector)\n",
    "\n",
    "\n",
    "        nearest_results = []\n",
    "        for img in img_to_search:\n",
    "            nearest = qd.get_top_x_similar_images(client, collection, top, img)\n",
    "            nearest_results.append(nearest)\n",
    "\n",
    "        st.markdown(\"## Results:\")\n",
    "\n",
    "        def print_results(results):\n",
    "            msg = f\"{len(results)} faces were found in the provided image\" if len(results) > 1 else f\"{len(results)} face was found in the provided image\"\n",
    "            st.markdown(msg)\n",
    "\n",
    "            for j, result in enumerate(results):\n",
    "                points = result.points\n",
    "                st.markdown(f\"Face {j+1}:\")\n",
    "                st.image(searched_faces[j], width = 200)\n",
    "                for i in range(len(points)):\n",
    "                    name = points[i].payload['name']\n",
    "                    score = points[i].score\n",
    "                    st.markdown(f\"Result #{i+1}: {name} was diagnosed with {score * 100} confidence\")\n",
    "                    st.markdown(f\"This image score was {score}\")\n",
    "                    st.image(f\"faces/{points[i].payload['file_name']}\")\n",
    "                    st.markdown(\"-\" * 50)\n",
    "\n",
    "\n",
    "        print_results(nearest_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run search_image.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este projeto constituiu uma experiência de aprendizagem valiosa, proporcionando-nos a oportunidade de aprofundar conhecimentos em diversas áreas tecnológicas. Tivemos a possibilidade de trabalhar com bases de dados vetoriais, explorando a sua estrutura e potencial para armazenamento e pesquisa de dados complexos. A utilização de modelos de *Machine Learning* para a transformação de imagens em *embeddings* permitiu-nos compreender o poder da representação vetorial na análise visual. Adicionalmente, aplicamos técnicas de processamento de imagem para preparar os dados e desenvolvemos uma aplicação web, recorrendo a bibliotecas e frameworks especializados, consolidando assim as nossas competências de desenvolvimento.\n",
    "\n",
    "Do nosso ponto de vista, o objetivo inicial de conceber um sistema de recolha e armazenamento de imagens num sistema de bases de dados vetorial, com a finalidade de identificar padrões visuais, foi efetivamente alcançado. O sistema implementado demonstra a capacidade de processar imagens, extrair características relevantes e armazená-las de forma que seja possível fazer pesquisas por imagens similares.\n",
    "\n",
    "Contudo, reconhecemos que há muito a explorar no sentido de refinar e expandir este projeto. Para evoluir para um sistema mais robusto, abrangente e fiável, poderíamos seguir por vários caminhos de desenvolvimento. Uma direção promissora seria treinar os modelos de extração de características. Ajustar os modelos aos nossos dados e à tarefa particular de reconhecimento facial poderia otimizar significativamente a qualidade dos *embeddings* gerados e, consequentemente, a precisão do sistema. Outra melhoria crucial seria tornar a aplicação mais interativa e flexível, permitindo ao utilizador fornecer uma imagem representativa de um padrão específico a ser procurado na base de dados. Esta funcionalidade expandiria significativamente a usabilidade do sistema para cenários de pesquisa mais direcionados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
