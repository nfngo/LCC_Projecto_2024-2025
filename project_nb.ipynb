{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "No âmbito da UC Projeto, foi-nos sugerida a criação de um sistema para angariação e armazenamento de ficheiros de imagem num sistema de bases de dados vetorial, para posterior identificação de padrões de imagens.\n",
    "\n",
    "No sentido de ir ao encontro do sugerido, decidimos criar um sistema de reconhecimento facial. Este sistema, permite-nos fornecer uma imagem como input, identifica as faces presentes na imagem e apresenta-nos como resultado as imagens das faces mais semelhantes ao input que foram encontradas na nossa base de dados.\n",
    "\n",
    "Ao longo deste documento iremos detalhar quais as tecnologias utilizadas, as etapas do desenvolvimento deste sistema e particularidades da sua implemntação.\n",
    "\n",
    "Por fim, faremos uma pequena demonstração do uso da aplicação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Contextualização e Motivação\n",
    "\n",
    "O reconhecimento facial é um campo de investigação e desenvolvimento ativo há várias décadas, com inúmeras aplicações já implementadas no nosso dia a dia. Desde o desbloqueio de smartphones até aos sistemas de vigilância, a tecnologia tem demonstrado o seu potencial e importância crescente.\n",
    "\n",
    "No entanto, existem vários desafios, sejam eles na identificação de características das faces ou no armazenamento das imagens numa base de dados. Lidar com variações na aparência facial devido a diferentes condições de iluminação, ângulos de visão, expressões faciais e oclusões (como óculos ou máscaras) continua a ser um problema complexo; as dificuldades que as bases de dados tradicionais têm no armazenamento de imagens é também um entrave: as imagens podem ser ficheiros grandes, e armazenar um grande volume delas diretamente na base de dados pode levar a um crescimento exponencial do tamanho da mesma, o que pode levar a um baixo desempenho aplicacional.\n",
    "\n",
    "É neste contexto que a utilização de bases de dados vetoriais surge como uma alternativa promissora. Ao representar imagens como vetores num espaço de alta dimensão, onde a distância entre vetores reflete a similaridade entre as imagens, torna-se possível realizar pesquisas mais eficientes e encontrar correspondências mesmo quando as imagens apresentam variações significativas.\n",
    "\n",
    "Este projeto vai de encontro às tendências atuais na área de reconhecimento de padrões e análise de imagem, explorando a cooperação entre técnicas de visão computacional e sistemas de armazenamento. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecnologias e Bibiliotecas\n",
    "\n",
    "IDEIAS GERAIS:\n",
    "- Linguagem utilizada: python\n",
    "- Base de dados: Qdrant -> O que nos levou a escolher esta BD? Boa documentação, fácil configuração, possibilidade de usar localmente recorrendo a uma imagem docker\n",
    "- Vision Transformers -> breve explicação de como funcionam com um exemplo mundano, quais escolhemos e porquê -- Fizemos a escolha baseada nos seguintes critérios: tarefa para o qual o modelo foi desenvolvido (Image Feature Extraction), tamanho do dataset com que foi treinado, empresas que os desenvolveram (google e facebook), resolução das imagens (em ambos 224x224) e dimensão do embedding gerado (ambos 768) - DATALOOP [1](https://dataloop.ai/library/model/)  e HUGGINGFACE [2](https://huggingface.co/models?pipeline_tag=image-feature-extraction)\n",
    "- MTCNN -> biblioteca robusta para detecção facial, desenvolvida para detetar faces e os seus pontos de referência numa imagem, utilzando uma MCCN (multitask cascaded convolutional network)\n",
    "- Outras bibliotecas para manipulação de dados numéricos e vetores e processamento de imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Configuração Inicial\n",
    "\n",
    "Antes de correr o código deste notebook, é necessário:\n",
    "- Fazer download e guardar as imagens a inserir na base de dados numa pasta com nome \"assets\"\n",
    "- Criar um ambiente virtual\n",
    "- Instalar todos os packages necessários\n",
    "- Garantir que temos a plataforma docker instalada e inicializar um container com Qdrant\n",
    "\n",
    "\n",
    "```Bash\n",
    "# Criar ambiente com conda\n",
    "conda env create -n my_env python=3.11 pip\n",
    "conda activate my_env\n",
    "\n",
    "# Instalar packages\n",
    "pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# Download da imagem de Qdrant\n",
    "docker pull qdrant/qdrant\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import qdrant.utils as qd\n",
    "import models.models as mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using containers\n",
    "# client = QdrantClient(host=\"qdrant\", port=6333)\n",
    "\n",
    "# Local dev\n",
    "client = QdrantClient(host=\"localhost\", port=6333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"image_collection\"\n",
    "\n",
    "# facebook/dino-vitb16 size: 768\n",
    "# Vit Base Patch16 224 In21k size: 768\n",
    "qd.create_collection(client, collection, 768, models.Distance.COSINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "extensions = (\".jpg\", \".jpeg\")\n",
    "\n",
    "def get_images_data(path):\n",
    "    data = []\n",
    "    for file in os.listdir(f\"{path}\"):\n",
    "        if file.endswith(extensions):\n",
    "            file_name = file.split(\".\")[0].lower().replace(\"_\",\" \")\n",
    "            name = \"\".join(c for c in file_name if c.isalpha() or c == ' ')[:-1] \n",
    "            img_data = {\n",
    "                \"name\": name,\n",
    "                \"file_name\": file,\n",
    "                \"img_obj\": Image.open(os.path.join(f\"{path}\",file))\n",
    "            }\n",
    "            data.append(img_data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = get_images_data(f\"{current_directory}/assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models \n",
    "dino = \"dino-vitb16\"\n",
    "device, processor, model, detector = mdl.init_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if needed\n",
    "faces_path = \"faces\"\n",
    "if not os.path.exists(faces_path):\n",
    "    os.makedirs(faces_path)\n",
    "\n",
    "\n",
    "destiny_path = f\"{current_directory}/{faces_path}/\"\n",
    "for i, image_data in enumerate(data):\n",
    "    mdl.process_image(image_data['img_obj'], \n",
    "                      detector,\n",
    "                      destiny_path,\n",
    "                      image_data['file_name'])\n",
    "\n",
    "faces_data = get_images_data(f\"{current_directory}/{faces_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_data, len(faces_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "embeddings = mdl.gen_embeddings(faces_data, processor, device, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to DB\n",
    "for img_data in embeddings:\n",
    "    qd.insert_image_embedding(client, collection, img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_search, searched_faces = mdl.gen_embedding_img_to_search(f\"{current_directory}/img_to_search/taskmaster.jpg\", processor, device, model, detector)\n",
    "img_to_search, searched_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_to_search), len(img_to_search[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search top X similar results\n",
    "top = 3\n",
    "nearest_results = []\n",
    "for img in img_to_search:\n",
    "    nearest = qd.get_top_x_similar_images(client, collection, top, img)\n",
    "    nearest_results.append(nearest)\n",
    "\n",
    "nearest_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    msg = f\"{len(results)} faces were found in the provided image\" if len(results) > 1 else f\"{len(results)} face was found in the provided image\"\n",
    "    print(msg)\n",
    "    for j, result in enumerate(results):\n",
    "        # print(f\"{j}: {result.points}\")\n",
    "        points = result.points\n",
    "        print(f\"Face {j+1}:\")\n",
    "        searched_faces[j].show()\n",
    "        for i in range(len(points)):\n",
    "            name = points[i].payload['name']\n",
    "            score = points[i].score\n",
    "            print(f\"Result #{i+1}: {name} was diagnosed with {score * 100} confidence\")\n",
    "            print(f\"This image score was {score}\")\n",
    "            Image.open(f\"faces/{points[i].payload['file_name']}\").show()\n",
    "            print(\"-\" * 50)\n",
    "            print()\n",
    "\n",
    "print_results(nearest_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data_to_add = [{\n",
    "   'img_path': 'img_to_add/João_Baião_1.jpg',\n",
    "   'payload': {\n",
    "      'name': 'João Baião',\n",
    "      'file_name': 'João_Baião_1.jpg'\n",
    "   }\n",
    "},\n",
    "{\n",
    "   'img_path': 'img_to_add/João_Baião_2.jpg',\n",
    "   'payload': {\n",
    "      'name': 'João Baião',\n",
    "      'file_name': 'João_Baião_2.jpg'\n",
    "   }\n",
    "},\n",
    "{\n",
    "   'img_path': 'img_to_add/João_Baião_3.jpg',\n",
    "   'payload': {\n",
    "      'name': 'João Baião',\n",
    "      'file_name': 'João_Baião_3.jpg'\n",
    "   }\n",
    "}]\n",
    "\n",
    "def add_img_to_db(data_to_add: list[dict]):\n",
    "   img_data = []\n",
    "   for data in data_to_add:\n",
    "      img_obj = Image.open(data['img_path'])\n",
    "      img_data.append({\n",
    "         'name': data['payload']['name'],\n",
    "         'file_name': data['payload']['file_name'],\n",
    "         'img_obj': img_obj\n",
    "      })\n",
    "   embeddings = mdl.gen_embeddings(img_data, processor, device, model)\n",
    "   for img in embeddings:\n",
    "      qd.insert_image_embedding(client, collection, img)\n",
    "   \n",
    "\n",
    "add_img_to_db(img_data_to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = qd.get_points_from_collection(client, collection, 5)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = ''\n",
    "qd.delete_points_from_collection(client, collection, [id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run search_image.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
